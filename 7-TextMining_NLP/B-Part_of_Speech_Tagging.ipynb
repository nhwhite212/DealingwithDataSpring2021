{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.5"
    },
    "colab": {
      "name": "B-Part_of_Speech_Tagging.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/nhwhite212/DealingwithDataSpring2021/blob/master/7-TextMining_NLP/B-Part_of_Speech_Tagging.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YmehlYj2gnMW"
      },
      "source": [
        "## Categorizing and Tagging Words"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MMr_xCD6gnMc"
      },
      "source": [
        "Back in elementary school you learnt the difference between nouns, verbs, adjectives, and adverbs. These are  very useful categories for many language processing tasks. Our goals chapter is to answer the following questions:\n",
        "\n",
        "1. What are lexical categories and how are they used in natural language processing?\n",
        "2. What is a good Python data structure for storing words and their categories?\n",
        "3. How can we automatically tag each word of a text with its word class?\n",
        "\n",
        "The process of classifying words into their parts of speech and labeling them accordingly is known as part-of-speech tagging, POS-tagging, or simply tagging. \n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P7qukUx3gnMd"
      },
      "source": [
        "### Using a POS tagger"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K2wbZwZJgnMd"
      },
      "source": [
        "A part-of-speech tagger, or POS-tagger, processes a sequence of words, and attaches a part of speech tag to each word:\n",
        "This is very important for trying to extract meaning from text. We often  need to find out the WHAT, WHERE, WHO and HOW in a document, or determine the sentiment of a document. The NLTK (Natural Language Tool Kit) library is one of a number of systems that we can use to understand text. Here are some examples:\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yepR8qyjjaUL",
        "outputId": "e79cc69e-01ee-4d81-ee63-74abafc65783"
      },
      "source": [
        "!pip  install --user -U nltk"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already up-to-date: nltk in /root/.local/lib/python3.7/site-packages (3.6.2)\n",
            "Requirement already satisfied, skipping upgrade: tqdm in /usr/local/lib/python3.7/dist-packages (from nltk) (4.41.1)\n",
            "Requirement already satisfied, skipping upgrade: click in /usr/local/lib/python3.7/dist-packages (from nltk) (7.1.2)\n",
            "Requirement already satisfied, skipping upgrade: joblib in /usr/local/lib/python3.7/dist-packages (from nltk) (1.0.1)\n",
            "Requirement already satisfied, skipping upgrade: regex in /usr/local/lib/python3.7/dist-packages (from nltk) (2019.12.20)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qhMQuQPognMe",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "cbd6a94c-895d-410d-b938-7a02d9715aa9"
      },
      "source": [
        "# load the toolkit\n",
        "import nltk\n",
        "nltk.download('popular')\n",
        "from nltk.tokenize import word_tokenize\n",
        "import inspect\n"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading collection 'popular'\n",
            "[nltk_data]    | \n",
            "[nltk_data]    | Downloading package cmudict to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/cmudict.zip.\n",
            "[nltk_data]    | Downloading package gazetteers to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/gazetteers.zip.\n",
            "[nltk_data]    | Downloading package genesis to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/genesis.zip.\n",
            "[nltk_data]    | Downloading package gutenberg to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/gutenberg.zip.\n",
            "[nltk_data]    | Downloading package inaugural to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/inaugural.zip.\n",
            "[nltk_data]    | Downloading package movie_reviews to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/movie_reviews.zip.\n",
            "[nltk_data]    | Downloading package names to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/names.zip.\n",
            "[nltk_data]    | Downloading package shakespeare to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/shakespeare.zip.\n",
            "[nltk_data]    | Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/stopwords.zip.\n",
            "[nltk_data]    | Downloading package treebank to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/treebank.zip.\n",
            "[nltk_data]    | Downloading package twitter_samples to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/twitter_samples.zip.\n",
            "[nltk_data]    | Downloading package omw to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/omw.zip.\n",
            "[nltk_data]    | Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/wordnet.zip.\n",
            "[nltk_data]    | Downloading package wordnet_ic to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/wordnet_ic.zip.\n",
            "[nltk_data]    | Downloading package words to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/words.zip.\n",
            "[nltk_data]    | Downloading package maxent_ne_chunker to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping chunkers/maxent_ne_chunker.zip.\n",
            "[nltk_data]    | Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]    |   Package punkt is already up-to-date!\n",
            "[nltk_data]    | Downloading package snowball_data to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    | Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping taggers/averaged_perceptron_tagger.zip.\n",
            "[nltk_data]    | \n",
            "[nltk_data]  Done downloading collection popular\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NbtP-8G_gnMf"
      },
      "source": [
        "Use the toolkit to tokenize (parse)some text into words, and then label the words with their parts of speech."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RxMbzdWkgnMf",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9ac5cc5a-ce84-4f30-9293-a5c149c56f76"
      },
      "source": [
        "# tokenize the text\n",
        "text = nltk.word_tokenize(\"And now for something completely different\")\n",
        "# Show the parts of speech for each word\n",
        "print(text)\n",
        "nltk.pos_tag(text)"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['And', 'now', 'for', 'something', 'completely', 'different']\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('And', 'CC'),\n",
              " ('now', 'RB'),\n",
              " ('for', 'IN'),\n",
              " ('something', 'NN'),\n",
              " ('completely', 'RB'),\n",
              " ('different', 'JJ')]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aKfBDxe7gnMh"
      },
      "source": [
        "OK, but what do 'CC', 'RB', 'IN', mean? Here we see that AND  is a CC, a coordinating conjunction; NOW and COMPLETELY are RB, or adverbs; FOR is an IN, a preposition; SOMETHING is NN, a noun; and DIFFERENT is JJ, an adjective.\n",
        "\n",
        "NLTK provides documentation for each tag, which can be queried using the tag, e.g. `nltk.help.upenn_tagset('RB')`, or a regular expression, e.g. `nltk.help.upenn_tagset('NN.*')`. First we have to downlaod the tagsets."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CFp6gkFCgnMi",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5990acc4-4150-4369-e442-ec38009fb6ea"
      },
      "source": [
        " nltk.download('tagsets')\n",
        "\n"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package tagsets to /root/nltk_data...\n",
            "[nltk_data]   Package tagsets is already up-to-date!\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 24
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fUbM6YWsgnMj"
      },
      "source": [
        "If the next command doesn't work, type nltk.download()\n",
        "and download the 'book' grammer, by typing'd' and then 'book'\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NuOKC2oMgnMj",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "102ed66c-1be7-424c-ad37-10af76fa9924"
      },
      "source": [
        "# ASK NLTK what a   JJ is, and some examples\n",
        "\n",
        "nltk.help.upenn_tagset('JJ')"
      ],
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "JJ: adjective or numeral, ordinal\n",
            "    third ill-mannered pre-war regrettable oiled calamitous first separable\n",
            "    ectoplasmic battery-powered participatory fourth still-to-be-named\n",
            "    multilingual multi-disciplinary ...\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "do9gXh73gnMk"
      },
      "source": [
        "### TAGSET meanings for the UPENN  (default) tagset.\n",
        "Display all of the possible POS tags and examples."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8hsWAOhqgnMk",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5717e134-26f0-4e44-f7fd-41b017182e30"
      },
      "source": [
        "nltk.help.upenn_tagset()"
      ],
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "$: dollar\n",
            "    $ -$ --$ A$ C$ HK$ M$ NZ$ S$ U.S.$ US$\n",
            "'': closing quotation mark\n",
            "    ' ''\n",
            "(: opening parenthesis\n",
            "    ( [ {\n",
            "): closing parenthesis\n",
            "    ) ] }\n",
            ",: comma\n",
            "    ,\n",
            "--: dash\n",
            "    --\n",
            ".: sentence terminator\n",
            "    . ! ?\n",
            ":: colon or ellipsis\n",
            "    : ; ...\n",
            "CC: conjunction, coordinating\n",
            "    & 'n and both but either et for less minus neither nor or plus so\n",
            "    therefore times v. versus vs. whether yet\n",
            "CD: numeral, cardinal\n",
            "    mid-1890 nine-thirty forty-two one-tenth ten million 0.5 one forty-\n",
            "    seven 1987 twenty '79 zero two 78-degrees eighty-four IX '60s .025\n",
            "    fifteen 271,124 dozen quintillion DM2,000 ...\n",
            "DT: determiner\n",
            "    all an another any both del each either every half la many much nary\n",
            "    neither no some such that the them these this those\n",
            "EX: existential there\n",
            "    there\n",
            "FW: foreign word\n",
            "    gemeinschaft hund ich jeux habeas Haementeria Herr K'ang-si vous\n",
            "    lutihaw alai je jour objets salutaris fille quibusdam pas trop Monte\n",
            "    terram fiche oui corporis ...\n",
            "IN: preposition or conjunction, subordinating\n",
            "    astride among uppon whether out inside pro despite on by throughout\n",
            "    below within for towards near behind atop around if like until below\n",
            "    next into if beside ...\n",
            "JJ: adjective or numeral, ordinal\n",
            "    third ill-mannered pre-war regrettable oiled calamitous first separable\n",
            "    ectoplasmic battery-powered participatory fourth still-to-be-named\n",
            "    multilingual multi-disciplinary ...\n",
            "JJR: adjective, comparative\n",
            "    bleaker braver breezier briefer brighter brisker broader bumper busier\n",
            "    calmer cheaper choosier cleaner clearer closer colder commoner costlier\n",
            "    cozier creamier crunchier cuter ...\n",
            "JJS: adjective, superlative\n",
            "    calmest cheapest choicest classiest cleanest clearest closest commonest\n",
            "    corniest costliest crassest creepiest crudest cutest darkest deadliest\n",
            "    dearest deepest densest dinkiest ...\n",
            "LS: list item marker\n",
            "    A A. B B. C C. D E F First G H I J K One SP-44001 SP-44002 SP-44005\n",
            "    SP-44007 Second Third Three Two * a b c d first five four one six three\n",
            "    two\n",
            "MD: modal auxiliary\n",
            "    can cannot could couldn't dare may might must need ought shall should\n",
            "    shouldn't will would\n",
            "NN: noun, common, singular or mass\n",
            "    common-carrier cabbage knuckle-duster Casino afghan shed thermostat\n",
            "    investment slide humour falloff slick wind hyena override subhumanity\n",
            "    machinist ...\n",
            "NNP: noun, proper, singular\n",
            "    Motown Venneboerger Czestochwa Ranzer Conchita Trumplane Christos\n",
            "    Oceanside Escobar Kreisler Sawyer Cougar Yvette Ervin ODI Darryl CTCA\n",
            "    Shannon A.K.C. Meltex Liverpool ...\n",
            "NNPS: noun, proper, plural\n",
            "    Americans Americas Amharas Amityvilles Amusements Anarcho-Syndicalists\n",
            "    Andalusians Andes Andruses Angels Animals Anthony Antilles Antiques\n",
            "    Apache Apaches Apocrypha ...\n",
            "NNS: noun, common, plural\n",
            "    undergraduates scotches bric-a-brac products bodyguards facets coasts\n",
            "    divestitures storehouses designs clubs fragrances averages\n",
            "    subjectivists apprehensions muses factory-jobs ...\n",
            "PDT: pre-determiner\n",
            "    all both half many quite such sure this\n",
            "POS: genitive marker\n",
            "    ' 's\n",
            "PRP: pronoun, personal\n",
            "    hers herself him himself hisself it itself me myself one oneself ours\n",
            "    ourselves ownself self she thee theirs them themselves they thou thy us\n",
            "PRP$: pronoun, possessive\n",
            "    her his mine my our ours their thy your\n",
            "RB: adverb\n",
            "    occasionally unabatingly maddeningly adventurously professedly\n",
            "    stirringly prominently technologically magisterially predominately\n",
            "    swiftly fiscally pitilessly ...\n",
            "RBR: adverb, comparative\n",
            "    further gloomier grander graver greater grimmer harder harsher\n",
            "    healthier heavier higher however larger later leaner lengthier less-\n",
            "    perfectly lesser lonelier longer louder lower more ...\n",
            "RBS: adverb, superlative\n",
            "    best biggest bluntest earliest farthest first furthest hardest\n",
            "    heartiest highest largest least less most nearest second tightest worst\n",
            "RP: particle\n",
            "    aboard about across along apart around aside at away back before behind\n",
            "    by crop down ever fast for forth from go high i.e. in into just later\n",
            "    low more off on open out over per pie raising start teeth that through\n",
            "    under unto up up-pp upon whole with you\n",
            "SYM: symbol\n",
            "    % & ' '' ''. ) ). * + ,. < = > @ A[fj] U.S U.S.S.R * ** ***\n",
            "TO: \"to\" as preposition or infinitive marker\n",
            "    to\n",
            "UH: interjection\n",
            "    Goodbye Goody Gosh Wow Jeepers Jee-sus Hubba Hey Kee-reist Oops amen\n",
            "    huh howdy uh dammit whammo shucks heck anyways whodunnit honey golly\n",
            "    man baby diddle hush sonuvabitch ...\n",
            "VB: verb, base form\n",
            "    ask assemble assess assign assume atone attention avoid bake balkanize\n",
            "    bank begin behold believe bend benefit bevel beware bless boil bomb\n",
            "    boost brace break bring broil brush build ...\n",
            "VBD: verb, past tense\n",
            "    dipped pleaded swiped regummed soaked tidied convened halted registered\n",
            "    cushioned exacted snubbed strode aimed adopted belied figgered\n",
            "    speculated wore appreciated contemplated ...\n",
            "VBG: verb, present participle or gerund\n",
            "    telegraphing stirring focusing angering judging stalling lactating\n",
            "    hankerin' alleging veering capping approaching traveling besieging\n",
            "    encrypting interrupting erasing wincing ...\n",
            "VBN: verb, past participle\n",
            "    multihulled dilapidated aerosolized chaired languished panelized used\n",
            "    experimented flourished imitated reunifed factored condensed sheared\n",
            "    unsettled primed dubbed desired ...\n",
            "VBP: verb, present tense, not 3rd person singular\n",
            "    predominate wrap resort sue twist spill cure lengthen brush terminate\n",
            "    appear tend stray glisten obtain comprise detest tease attract\n",
            "    emphasize mold postpone sever return wag ...\n",
            "VBZ: verb, present tense, 3rd person singular\n",
            "    bases reconstructs marks mixes displeases seals carps weaves snatches\n",
            "    slumps stretches authorizes smolders pictures emerges stockpiles\n",
            "    seduces fizzes uses bolsters slaps speaks pleads ...\n",
            "WDT: WH-determiner\n",
            "    that what whatever which whichever\n",
            "WP: WH-pronoun\n",
            "    that what whatever whatsoever which who whom whosoever\n",
            "WP$: WH-pronoun, possessive\n",
            "    whose\n",
            "WRB: Wh-adverb\n",
            "    how however whence whenever where whereby whereever wherein whereof why\n",
            "``: opening quotation mark\n",
            "    ` ``\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4r9mEKNygnMl"
      },
      "source": [
        "### Representing Tagged Tokens"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jxZu0SoSgnMl"
      },
      "source": [
        "By convention in NLTK, a tagged token is represented using a **tuple** consisting of the token and the tag. We can create one of these special tuples from the standard string representation of a tagged token, using the function str2tuple():"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C9_XoDyignMl"
      },
      "source": [
        "Note how NLTK treats (disambiguates)  the two occurences of the token \"refuse\" in the sentence below."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aNRkUHccgnMl",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3eba4035-9e3f-4c25-82a4-ab2846c57605"
      },
      "source": [
        "tokens = nltk.word_tokenize(\"They refuse to permit us to obtain the refuse permit\")\n",
        "tagged = nltk.pos_tag(tokens)\n",
        "tagged"
      ],
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('They', 'PRP'),\n",
              " ('refuse', 'VBP'),\n",
              " ('to', 'TO'),\n",
              " ('permit', 'VB'),\n",
              " ('us', 'PRP'),\n",
              " ('to', 'TO'),\n",
              " ('obtain', 'VB'),\n",
              " ('the', 'DT'),\n",
              " ('refuse', 'NN'),\n",
              " ('permit', 'NN')]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 27
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n0ovHcyTgnMm"
      },
      "source": [
        "We can index into the \"tagged\" tuple and retrieve the first element."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "l4Ba_HO6gnMm",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "856ee10c-feb9-44be-f5ec-d1a8ee777062"
      },
      "source": [
        "tagged_token = tagged[0]\n",
        "print(tagged_token)\n",
        "print(tagged_token[0])\n",
        "print(tagged_token[1])"
      ],
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "('They', 'PRP')\n",
            "They\n",
            "PRP\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m4fDgA80gnMm"
      },
      "source": [
        "Now lets iterate through the tagged tuples and break out the token and the POS Tag.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "v0hT0HaVgnMn",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0b529ada-59e4-40bc-f2f0-f30025b6d79f"
      },
      "source": [
        "# print the original text, tokenized\n",
        "print(\"Text = \", tokens)\n",
        "# Now the same from the tagged tuples (note the list comprehension)\n",
        "tokens = [a for (a, b) in tagged]\n",
        "print(\"Tokens = \",tokens)\n",
        "# and then print the POS TAGs\n",
        "tags = [b for (a, b) in tagged]\n",
        "print(\"POS Tags = \", tags)"
      ],
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Text =  ['They', 'refuse', 'to', 'permit', 'us', 'to', 'obtain', 'the', 'refuse', 'permit']\n",
            "Tokens =  ['They', 'refuse', 'to', 'permit', 'us', 'to', 'obtain', 'the', 'refuse', 'permit']\n",
            "POS Tags =  ['PRP', 'VBP', 'TO', 'VB', 'PRP', 'TO', 'VB', 'DT', 'NN', 'NN']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cpDsE9uugnMn"
      },
      "source": [
        "#### Exercise \n",
        "\n",
        "Load a text of your choice, tokenize it, and perform part of speech tagging on it. Then extract the nouns from the text, and perform a frequency anaysis, to identify the most common nouns in the text. (Warning: POS tagging takes a good amount of time when processing long texts, so try to select a text with less than 10K tokens, or simply perform POS tagging on the first 10K-20K tokens).\n",
        "\n",
        "Repeat the exercise for adjectives.\n",
        "\n",
        "PS: If you want to parse text from HTML without resorting to XPath expressions, you can use the \"BeautifulSoup\" library:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B7u3MdFRgnMq"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lSUiyWbTgnMq"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XNmspMcignMq"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VqnlYjFTgnMq"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}